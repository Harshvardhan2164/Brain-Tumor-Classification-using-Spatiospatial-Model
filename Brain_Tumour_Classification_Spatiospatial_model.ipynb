{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T07:28:19.956489Z",
     "iopub.status.busy": "2024-12-13T07:28:19.956207Z",
     "iopub.status.idle": "2024-12-13T07:28:31.457746Z",
     "shell.execute_reply": "2024-12-13T07:28:31.456633Z",
     "shell.execute_reply.started": "2024-12-13T07:28:19.956462Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torchio seaborn --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T08:48:03.199780Z",
     "iopub.status.busy": "2024-12-13T08:48:03.199438Z",
     "iopub.status.idle": "2024-12-13T08:48:03.205056Z",
     "shell.execute_reply": "2024-12-13T08:48:03.204233Z",
     "shell.execute_reply.started": "2024-12-13T08:48:03.199750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchio as tio\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import getcwd\n",
    "from pathlib import Path\n",
    "import torchio\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T07:30:18.034817Z",
     "iopub.status.busy": "2024-12-13T07:30:18.034511Z",
     "iopub.status.idle": "2024-12-13T07:36:54.582132Z",
     "shell.execute_reply": "2024-12-13T07:36:54.581239Z",
     "shell.execute_reply.started": "2024-12-13T07:30:18.034792Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class R2Plus1dStem4MRI(nn.Module):\n",
    "    def __init__(self, num_classes=3):  # num_classes = 3 (HGG, LGG, IXI)\n",
    "        super(R2Plus1dStem4MRI, self).__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv3d(1, 45, kernel_size=(1, 7, 7),\n",
    "                      stride=(1, 2, 2), padding=(0, 3, 3),\n",
    "                      bias=False),\n",
    "            nn.BatchNorm3d(45),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(45, 64, kernel_size=(3, 1, 1),\n",
    "                      stride=(1, 1, 1), padding=(1, 0, 0),\n",
    "                      bias=False),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)  # Output shape will be [batch_size, 64, 1, 1, 1]\n",
    "        # Fully connected layer to output class scores\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.global_pool(x)  # Reduce to [batch_size, 64, 1, 1, 1]\n",
    "        x = x.view(x.size(0), -1)  # Flatten to [batch_size, 64]\n",
    "        x = self.fc(x)  # Output shape will be [batch_size, num_classes]\n",
    "        return x\n",
    "\n",
    "# Define the dataset\n",
    "class Datasets:\n",
    "    def __init__(self, dataset_path, ixi_path, target_shape=(240, 240, 155)):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.ixi_path = ixi_path\n",
    "        self.target_shape = target_shape\n",
    "        self.transform = tio.CropOrPad(target_shape)\n",
    "\n",
    "    def brats(self):\n",
    "        HGG = []  # High-grade glioma samples\n",
    "        for path, _, files in os.walk(os.path.join(self.dataset_path, 'MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/')):\n",
    "            for file in files:\n",
    "                if file.endswith('_t1ce.nii'):\n",
    "                    img_path = Path(path + '/' + file)\n",
    "                    print(f\"Loading HGG sample: {img_path}\")\n",
    "                    subject = tio.Subject(t1=tio.ScalarImage(img_path), label=1)\n",
    "                    subject = self.transform(subject)\n",
    "                    HGG.append(subject)\n",
    "        \n",
    "        LGG = []  # Low-grade glioma samples\n",
    "        for path, _, files in os.walk(os.path.join(self.dataset_path, 'MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/')):\n",
    "            for file in files:\n",
    "                if file.endswith('_t1ce.nii'):\n",
    "                    img_path = Path(path + '/' + file)\n",
    "                    print(f\"Loading LGG sample: {img_path}\")\n",
    "                    subject = tio.Subject(t1=tio.ScalarImage(img_path), label=0)\n",
    "                    subject = self.transform(subject)\n",
    "                    LGG.append(subject)\n",
    "\n",
    "        brats = HGG+LGG\n",
    "        \n",
    "        return brats  # Combine HGG and LGG\n",
    "    \n",
    "    def ixi(self):\n",
    "         ixi = [] # Healthy samples\n",
    "         for path, _, files in os.walk(self.ixi_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.nii'):\n",
    "                    img_path = Path(path + '/' + file)\n",
    "                    print(f\"Loading IXI sample: {img_path}\")\n",
    "                    subject = tio.Subject(t1=tio.ScalarImage(img_path), label=2)\n",
    "                    subject = self.transform(subject)\n",
    "                    ixi.append(subject)\n",
    "         return ixi\n",
    "        \n",
    "\n",
    "    def return_total_samples(self):\n",
    "        return self.ixi() + self.brats()\n",
    "\n",
    "# Data loader preparation\n",
    "def get_data_loaders(dataset, batch_size=1):\n",
    "    subjects_dataset = tio.SubjectsDataset(dataset.return_total_samples())\n",
    "    data_loader = DataLoader(subjects_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "# Training loop for 5 samples\n",
    "def train_model(model, data_loader, num_samples=1, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Track the losses\n",
    "    loss_values = []\n",
    "\n",
    "    model.train()\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        inputs = batch['t1'][tio.DATA].to(device).float()  # Convert input to FloatTensor\n",
    "        labels = batch['label'].to(device).long()  # Ensure labels are LongTensor for classification\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the loss\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "        # Print loss for each sample\n",
    "        print(f'Sample {i + 1}/{num_samples}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Visualize slices from this batch\n",
    "        visualize_slices(inputs.cpu().detach())\n",
    "\n",
    "# Visualization function remains the same\n",
    "def visualize_slices(inputs):\n",
    "    # inputs should be in the shape [batch_size, channels, depth, height, width]\n",
    "    batch_size, channels, depth, height, width = inputs.shape\n",
    "    for i in range(depth):\n",
    "        plt.imshow(inputs[0, 0, i, :, :], cmap='gray')\n",
    "        plt.title(f'Slice {i + 1}')\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset_path_1 = '/kaggle/input/miccaibrats2019'\n",
    "dataset_path_2 = '/kaggle/input/ixi-t1'\n",
    "dataset = Datasets(dataset_path_1, dataset_path_2)\n",
    "data_loader = get_data_loaders(dataset, batch_size=1)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = R2Plus1dStem4MRI(num_classes=3)\n",
    "train_model(model, data_loader, num_samples=1, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T04:30:02.902298Z",
     "iopub.status.busy": "2024-12-11T04:30:02.901966Z",
     "iopub.status.idle": "2024-12-11T04:30:02.910865Z",
     "shell.execute_reply": "2024-12-11T04:30:02.909656Z",
     "shell.execute_reply.started": "2024-12-11T04:30:02.902269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Datasets:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "    def brats(self):\n",
    "        HGG = []  # High-grade glioma samples\n",
    "        for path, _, files in os.walk(os.path.join(self.dataset_path, 'MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/HGG/')):\n",
    "            for file in files:\n",
    "                if file.endswith('_t1ce.nii'):\n",
    "                    img_path = Path(path + '/' + file)\n",
    "                    print(f\"Loading HGG sample: {img_path}\")  # Print the image path\n",
    "                    HGG.append(tio.Subject(t1=tio.ScalarImage(img_path), label=1,))\n",
    "        \n",
    "        LGG = []  # Low-grade glioma samples\n",
    "        for path, _, files in os.walk(os.path.join(self.dataset_path, 'MICCAI_BraTS_2019_Data_Training/MICCAI_BraTS_2019_Data_Training/LGG/')):\n",
    "            for file in files:\n",
    "                if file.endswith('_t1ce.nii'):\n",
    "                    img_path = Path(path + '/' + file)\n",
    "                    LGG.append(tio.Subject(t1=tio.ScalarImage(img_path), label=0,))\n",
    "\n",
    "        brats = HGG+LGG\n",
    "        \n",
    "        return brats\n",
    "\n",
    "    def ixi(self):\n",
    "        ixi = []  # Healthy samples\n",
    "        for path, currentDirectory, files in os.walk(os.path.join(self.dataset_path, '/kaggle/input/ixit2')):\n",
    "            for file in files:\n",
    "                if file.endswith('.nii'):\n",
    "                    img_path = Path(path + '/' + file)\n",
    "                    ixi.append(tio.Subject(t1=tio.ScalarImage(img_path), label=2,))\n",
    "        return ixi\n",
    "\n",
    "    def return_total_samples(self):\n",
    "        return self.brats()+self.ixi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet(2+1)D Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T07:37:01.303150Z",
     "iopub.status.busy": "2024-12-13T07:37:01.302067Z",
     "iopub.status.idle": "2024-12-13T07:37:01.317765Z",
     "shell.execute_reply": "2024-12-13T07:37:01.316894Z",
     "shell.execute_reply.started": "2024-12-13T07:37:01.303110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Sequential\n",
    "\n",
    "class R2Plus1dStem4MRI(nn.Sequential):\n",
    "    def __init__(self, num_classes=3):  # 3 classes: HGG, LGG, Healthy\n",
    "        super(R2Plus1dStem4MRI, self).__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv3d(1, 45, kernel_size=(1, 7, 7),\n",
    "                      stride=(1, 2, 2), padding=(0, 3, 3), bias=False),\n",
    "            nn.BatchNorm3d(45),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(45, 64, kernel_size=(3, 1, 1),\n",
    "                      stride=(1, 1, 1), padding=(1, 0, 0), bias=False),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)  # Global pooling\n",
    "        self.fc = nn.Linear(64, num_classes)  # Fully connected for class scores\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet3D Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T06:58:15.609594Z",
     "iopub.status.busy": "2024-12-11T06:58:15.609043Z",
     "iopub.status.idle": "2024-12-11T06:58:15.830070Z",
     "shell.execute_reply": "2024-12-11T06:58:15.829279Z",
     "shell.execute_reply.started": "2024-12-11T06:58:15.609557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 3D Basic Block\n",
    "class BasicBlock3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# 3D ResNet Model\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=3):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model (e.g., ResNet-18)\n",
    "def resnet3d(num_classes=3):\n",
    "    return ResNet3D(BasicBlock3D, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "model = resnet3d(num_classes=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for ResNet(2+1)D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T07:37:06.917674Z",
     "iopub.status.busy": "2024-12-13T07:37:06.916855Z",
     "iopub.status.idle": "2024-12-13T07:37:06.921580Z",
     "shell.execute_reply": "2024-12-13T07:37:06.920750Z",
     "shell.execute_reply.started": "2024-12-13T07:37:06.917639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_data_loaders(dataset, batch_size=2):\n",
    "    subjects_dataset = tio.SubjectsDataset(dataset.return_total_samples())\n",
    "    return DataLoader(subjects_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T07:37:32.172502Z",
     "iopub.status.busy": "2024-12-13T07:37:32.171931Z",
     "iopub.status.idle": "2024-12-13T07:37:32.186836Z",
     "shell.execute_reply": "2024-12-13T07:37:32.185974Z",
     "shell.execute_reply.started": "2024-12-13T07:37:32.172468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, num_epochs=10, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_values = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            inputs = batch['t1'][tio.DATA].to(device).float()\n",
    "            labels = batch['label'].to(device).long()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "            all_outputs.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "            print(f\"Batch {batch_idx + 1}/{len(data_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(data_loader)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")\n",
    "        loss_values.append(avg_loss)\n",
    "\n",
    "    torch.save(model.state_dict(), '/kaggle/working/r2plus1d_mri_model.pth')\n",
    "    print(\"Model saved as 'r2plus1d_mri_model.pth'\")\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['LGG', 'HGG', 'Healthy']))\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['LGG', 'HGG', 'Healthy'], yticklabels=['LGG', 'HGG', 'Healthy'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(loss_values, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs, axis=0)  # Convert list of arrays to a single array\n",
    "    all_labels_one_hot = label_binarize(all_labels, classes=[0, 1, 2])  # One-hot encode labels for ROC calculation\n",
    "\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    for i in range(3):  # Loop over classes (0: LGG, 1: HGG, 2: Healthy)\n",
    "        fpr[i], tpr[i], _ = roc_curve(all_labels_one_hot[:, i], all_outputs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Micro-average ROC curve\n",
    "    fpr['micro'], tpr['micro'], _ = roc_curve(all_labels_one_hot.ravel(), all_outputs.ravel())\n",
    "    roc_auc['micro'] = auc(fpr['micro'], tpr['micro'])\n",
    "\n",
    "    # Macro-average ROC curve\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(3)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(3):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= 3\n",
    "    fpr['macro'] = all_fpr\n",
    "    tpr['macro'] = mean_tpr\n",
    "    roc_auc['macro'] = auc(fpr['macro'], tpr['macro'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr['micro'], tpr['micro'], label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})', linestyle=':')\n",
    "    plt.plot(fpr['macro'], tpr['macro'], label=f'macro-average ROC curve (area = {roc_auc[\"macro\"]:.2f})', linestyle=':')\n",
    "\n",
    "    for i, class_name in enumerate(['LGG', 'HGG', 'Healthy']):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {class_name} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T08:48:25.377205Z",
     "iopub.status.busy": "2024-12-13T08:48:25.376874Z",
     "iopub.status.idle": "2024-12-13T09:05:06.777508Z",
     "shell.execute_reply": "2024-12-13T09:05:06.776733Z",
     "shell.execute_reply.started": "2024-12-13T08:48:25.377174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "model = R2Plus1dStem4MRI(num_classes=3)\n",
    "train_model(model, data_loader, num_epochs=3, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for ResNet3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T07:00:46.322264Z",
     "iopub.status.busy": "2024-12-11T07:00:46.321556Z",
     "iopub.status.idle": "2024-12-11T07:00:46.332560Z",
     "shell.execute_reply": "2024-12-11T07:00:46.331635Z",
     "shell.execute_reply.started": "2024-12-11T07:00:46.322229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model3(model, data_loader, num_epochs=5, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_values = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            inputs = batch['t1'][tio.DATA].to(device).float()\n",
    "            labels = batch['label'].to(device).long()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "            all_outputs.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "            print(f\"Batch {batch_idx + 1}/{len(data_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(data_loader)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")\n",
    "        loss_values.append(avg_loss)\n",
    "\n",
    "    torch.save(model.state_dict(), '/kaggle/working/r3d_mri_model.pth')\n",
    "    print(\"Model saved as 'r3d_mri_model.pth'\")\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['LGG', 'HGG', 'Healthy']))\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['LGG', 'HGG', 'Healthy'], yticklabels=['LGG', 'HGG', 'Healthy'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(loss_values, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs, axis=0)  # Convert list of arrays to a single array\n",
    "    all_labels_one_hot = label_binarize(all_labels, classes=[0, 1, 2])  # One-hot encode labels for ROC calculation\n",
    "\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    for i in range(3):  # Loop over classes (0: LGG, 1: HGG, 2: Healthy)\n",
    "        fpr[i], tpr[i], _ = roc_curve(all_labels_one_hot[:, i], all_outputs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Micro-average ROC curve\n",
    "    fpr['micro'], tpr['micro'], _ = roc_curve(all_labels_one_hot.ravel(), all_outputs.ravel())\n",
    "    roc_auc['micro'] = auc(fpr['micro'], tpr['micro'])\n",
    "\n",
    "    # Macro-average ROC curve\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(3)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(3):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= 3\n",
    "    fpr['macro'] = all_fpr\n",
    "    tpr['macro'] = mean_tpr\n",
    "    roc_auc['macro'] = auc(fpr['macro'], tpr['macro'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr['micro'], tpr['micro'], label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})', linestyle=':')\n",
    "    plt.plot(fpr['macro'], tpr['macro'], label=f'macro-average ROC curve (area = {roc_auc[\"macro\"]:.2f})', linestyle=':')\n",
    "\n",
    "    for i, class_name in enumerate(['LGG', 'HGG', 'Healthy']):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {class_name} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T07:00:53.295828Z",
     "iopub.status.busy": "2024-12-11T07:00:53.295211Z",
     "iopub.status.idle": "2024-12-11T07:07:05.314008Z",
     "shell.execute_reply": "2024-12-11T07:07:05.312986Z",
     "shell.execute_reply.started": "2024-12-11T07:00:53.295788Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "model = resnet3d(num_classes=3)\n",
    "train_model3(model, data_loader, num_epochs=3, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1255704,
     "sourceId": 2094015,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1551656,
     "sourceId": 2557428,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3233297,
     "sourceId": 5623819,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6270067,
     "sourceId": 10155547,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
